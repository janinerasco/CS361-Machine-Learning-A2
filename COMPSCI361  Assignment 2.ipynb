{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPSCI 361 ASSIGNMENT TWO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries and Dataset\n",
    "\n",
    "* Load the dataset and deal with missing values in an appropriate manner. Describe how you handled them and why you did it that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Libraries\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from autorank import autorank, plot_stats, create_report, latex_table\n",
    "from statistics import stdev\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# Render matplotlib plots in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Datasets\n",
    "X = pd.read_csv('data_A2.csv')\n",
    "y = pd.read_csv('labels_A2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to imputation, we would split the data into training and test data so we avoid indirectly informing the training set about the test set and maintain a distinct separation. However, for this assignment we ignore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-Test Split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data\n",
    "\n",
    "The best imputation strategy depends on the data. However, in the given case, we hypothesise that the best method for us to use is a measure of central tendency to replace missing data as we retain the same shape and do not lose data, whilst replacing domain knowledge with given data that would not require the  knowledge of an expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 4: Use a measure of Central Tendency for the Missing attributes\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Fill with mean\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "\n",
    "imputer.fit(X)\n",
    "\n",
    "X = imputer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = np.isnan(X)\n",
    "check.any(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "* Determine the 10 most important features of the cleaned dataset, explain how you found them and why you think they are important. This is the dataset you will be using for the rest of the assignment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 8, 11, 46, 60, 65, 69, 77, 78, 97]\n"
     ]
    }
   ],
   "source": [
    "#Univariate feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from numpy import transpose\n",
    "\n",
    "features = SelectKBest(score_func = f_classif)\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "fit = features.fit(X, np.ravel(y))\n",
    "imp_features = X.columns[features.get_support()].to_list()\n",
    "print(imp_features)\n",
    "\n",
    "for col in X.columns:\n",
    "    if col not in imp_features:\n",
    "        X = X.drop([col], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is done to reduce the amount of attributes and regulate dimensionality by eliminating features that are irrelevant, redundant or less useful than other features. Univariate selection works by selecting the best features based on univariate statistical tests and removing all but the highest-scoring features and in particular, we used the f_classif argument for classification. This method is based on the F-test and estimates the degree of linear dependency between two random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers\n",
    "\n",
    "* Get results for RF, pruned DT (make sure to use a validation set for pruning!), unpruned DT, and decision stumps and determine if any are statistically significantly better than others. Why are the worst methods performing so badly compared to the others?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 1234 #Random State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=1234)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest\n",
    "rf = RandomForestClassifier(random_state = rs)\n",
    "rf.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=1234)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unpruned Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state = rs)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=1, random_state=1234)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision stump\n",
    "dts = DecisionTreeClassifier(max_depth = 1, random_state = rs)\n",
    "dts.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruned Decision Tree\n",
    "path = DecisionTreeClassifier().cost_complexity_pruning_path(X_train, y_train)\n",
    "alphas = path['ccp_alphas']\n",
    "\n",
    "\n",
    "acc_val = []\n",
    "for c in path.ccp_alphas:\n",
    "    #X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)\n",
    "    acc_val.append(DecisionTreeClassifier(ccp_alpha=c).fit(X_train, y_train).score(X_val, y_val))\n",
    "\n",
    "best_ccp_alpha = path.ccp_alphas[np.argmax(acc_val)]\n",
    "best_clf = DecisionTreeClassifier(ccp_alpha=best_ccp_alpha).fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy on train set: 1.00\n",
      "Decision Tree Accuracy on test set: 0.57\n",
      "\n",
      "Random Forest Accuracy on train set: 1.00\n",
      "Random Forest Accuracy on test set: 0.68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Scores\n",
    "\n",
    "#Unpruned Decision Tree Score\n",
    "dt_score = dt.score(X_train, y_train)\n",
    "print(f'Decision Tree Accuracy on train set: {dt_score:.2f}')\n",
    "\n",
    "dt_score = dt.score(X_test, y_test)\n",
    "print(f'Decision Tree Accuracy on test set: {dt_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Random Forest Score\n",
    "rf_score = rf.score(X_train, y_train)\n",
    "print(f'Random Forest Accuracy on train set: {rf_score:.2f}')\n",
    "\n",
    "rf_score = rf.score(X_test, y_test)\n",
    "print(f'Random Forest Accuracy on test set: {rf_score:.2f}')\n",
    "print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Stump Accuracy on train set: 0.62\n",
      "Decision Stump Accuracy on test set: 0.59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Stump Score\n",
    "dts_score = dts.score(X_train, y_train)\n",
    "print(f'Decision Stump Accuracy on train set: {dts_score:.2f}')\n",
    "\n",
    "dts_score = dts.score(X_test, y_test)\n",
    "print(f'Decision Stump Accuracy on test set: {dts_score:.2f}')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Decision Tree Accuracy on train set: 0.97\n",
      "Pruned Decision Tree Accuracy on validation set: 0.47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pruned Decision Tree\n",
    "pdt_score = best_clf.score(X_train, y_train)\n",
    "print(f'Pruned Decision Tree Accuracy on train set: {pdt_score:.2f}')\n",
    "\n",
    "pdt_score = best_clf.score(X_val, y_test)\n",
    "print(f'Pruned Decision Tree Accuracy on validation set: {pdt_score:.2f}')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Mean test accuracy: 0.612\n",
      "Decision Tree Standard Deviation: 0.047\n",
      "\n",
      "Random Forest Mean test accuracy: 0.690\n",
      "Random Forest Standard Deviation: 0.044\n",
      "\n",
      "Decision Stump Mean test accuracy: 0.608\n",
      "Decision Stump Standard Deviation: 0.028\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cross Validation\n",
    "means = []\n",
    "sds = []\n",
    "y = np.ravel(y)\n",
    "\n",
    "#Unpruned Decision Trees\n",
    "dt_scores = cross_val_score(DecisionTreeClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(dt_scores)\n",
    "print(f\"Decision Tree Mean test accuracy: {np.mean(dt_scores):.3f}\")\n",
    "means += [dt_scores]\n",
    "\n",
    "std = stdev(dt_scores)\n",
    "sds += [std]\n",
    "print(f\"Decision Tree Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Random Forest\n",
    "rf_scores = cross_val_score(RandomForestClassifier(random_state = rs), X, y, cv=KFold(10))\n",
    "#print(rf_scores)\n",
    "print(f\"Random Forest Mean test accuracy: {np.mean(rf_scores):.3f}\")\n",
    "means += [rf_scores]\n",
    "\n",
    "std = stdev(rf_scores)\n",
    "sds += [std]\n",
    "print(f\"Random Forest Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Decision Stumps\n",
    "dts_scores = cross_val_score(DecisionTreeClassifier(max_depth = 1, random_state = rs), X, y, cv=KFold(10))\n",
    "#print(dt_scores)\n",
    "print(f\"Decision Stump Mean test accuracy: {np.mean(dts_scores):.3f}\")\n",
    "means += [dts_scores]\n",
    "\n",
    "std = stdev(dts_scores)\n",
    "sds += [std]\n",
    "print(f\"Decision Stump Standard Deviation: {std:.3f}\")\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(X), type(y))\n",
    "y = pd.DataFrame(y)\n",
    "#print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Decision Tree Mean test accuracy: 0.6329999999999999\n",
      "Pruned Decision Tree Standard Deviation: 0.040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pruned Decision Tree\n",
    "kf = KFold(n_splits = 10, shuffle = True)\n",
    "pdt = DecisionTreeClassifier(ccp_alpha = best_ccp_alpha, random_state = rs)\n",
    "\n",
    "pdt_scores = []\n",
    "for i in range(10):\n",
    "    result = next(kf.split(X), None)\n",
    "    X_train = X.iloc[result[0]]\n",
    "    X_test = X.iloc[result[1]]\n",
    "    y_train = y.iloc[result[0]]\n",
    "    y_test = y.iloc[result[1]]\n",
    "    model = pdt.fit(X_train, y_train)\n",
    "    predictions = pdt.predict(X_test)\n",
    "    pdt_scores.append(model.score(X_test, y_test))\n",
    "\n",
    "#print('Scores from each Iteration: ', pdt_scores)\n",
    "print('Pruned Decision Tree Mean test accuracy:', np.mean(pdt_scores))\n",
    "means += [np.mean(pdt_scores)]\n",
    "\n",
    "std = stdev(pdt_scores)\n",
    "sds += [std]\n",
    "print(f\"Pruned Decision Tree Standard Deviation: {std:.3f}\")\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                      meanrank      mean       std  ci_lower  ci_upper  \\\n",
      "Random Forest              1.7  0.695457  0.098841  0.598221  0.792693   \n",
      "Pruned Decision Tree       2.6  0.638364  0.052219  0.586993  0.689735   \n",
      "Decision Stumps            2.7  0.620456  0.033515  0.587485  0.653426   \n",
      "Decision Tree              3.0  0.604572  0.090419  0.515621  0.693522   \n",
      "\n",
      "                     effect_size   magnitude  \n",
      "Random Forest                  0  negligible  \n",
      "Pruned Decision Tree    0.722281      medium  \n",
      "Decision Stumps          1.01629       large  \n",
      "Decision Tree           0.959484       large  \n",
      "pvalue=0.13050077008525457\n",
      "cd=1.483221853685529\n",
      "omnibus=friedman\n",
      "posthoc=nemenyi\n",
      "all_normal=True\n",
      "pvals_shapiro=[0.3560923635959625, 0.15305356681346893, 0.2787017226219177, 0.9948100447654724]\n",
      "homoscedastic=False\n",
      "pval_homogeneity=0.01158797227943928\n",
      "homogeneity_test=bartlett\n",
      "alpha=0.05\n",
      "alpha_normality=0.0125\n",
      "num_samples=10\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=cohen_d)\n"
     ]
    }
   ],
   "source": [
    "#Autorank\n",
    "classifiers = ['Decision Tree', 'Random Forest', 'Decision Stumps', 'Pruned Decision Tree']\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for i in range(4):\n",
    "     data[classifiers[i]] = np.random.normal(means[i], sds[i], 10).clip(0, 1)\n",
    "\n",
    "result = autorank(data, alpha=0.05, verbose=False, approach = 'frequentist')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive Noise \n",
    "* Add 20% normal additive noise to the features and train the classifiers from step 3 and determine if any are performing significantly worse/better than on the clean dataset. Give reasons based on your knowledge of the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additive normal noise\n",
    "noise = np.random.normal(0, 0.2, np.shape(X))\n",
    "X_noise = X + np.multiply(noise, np.average(X, axis=0))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=1, random_state=5678)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = 5678\n",
    "\n",
    "#Train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_noise, y, test_size=0.3, random_state = rs)\n",
    "\n",
    "#Random Forest\n",
    "rf = RandomForestClassifier(random_state = rs)\n",
    "rf.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "#Unpruned Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state = rs)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#Decision stump\n",
    "dts = DecisionTreeClassifier(max_depth = 1, random_state = rs)\n",
    "dts.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy on train set: 1.00\n",
      "Decision Tree Accuracy on test set: 0.61\n",
      "\n",
      "Random Forest Accuracy on train set: 1.00\n",
      "Random Forest Accuracy on test set: 0.68\n",
      "\n",
      "Decision Stump Accuracy on train set: 0.64\n",
      "Decision Stump Accuracy on test set: 0.58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Scores\n",
    "\n",
    "#Unpruned Decision Tree Score\n",
    "dt_score = dt.score(X_train, y_train)\n",
    "print(f'Decision Tree Accuracy on train set: {dt_score:.2f}')\n",
    "\n",
    "dt_score = dt.score(X_test, y_test)\n",
    "print(f'Decision Tree Accuracy on test set: {dt_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Random Forest Score\n",
    "rf_score = rf.score(X_train, y_train)\n",
    "print(f'Random Forest Accuracy on train set: {rf_score:.2f}')\n",
    "\n",
    "rf_score = rf.score(X_test, y_test)\n",
    "print(f'Random Forest Accuracy on test set: {rf_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Decision Stump Score\n",
    "dts_score = dts.score(X_train, y_train)\n",
    "print(f'Decision Stump Accuracy on train set: {dts_score:.2f}')\n",
    "\n",
    "dts_score = dts.score(X_test, y_test)\n",
    "print(f'Decision Stump Accuracy on test set: {dts_score:.2f}')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Mean test accuracy: 0.585\n",
      "Decision Tree Standard Deviation: 0.060\n",
      "\n",
      "Random Forest Mean test accuracy: 0.699\n",
      "Random Forest Standard Deviation: 0.038\n",
      "\n",
      "Decision Stump Mean test accuracy: 0.603\n",
      "Decision Stump Standard Deviation: 0.032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cross Validation\n",
    "means = []\n",
    "sds = []\n",
    "y = np.ravel(y)\n",
    "\n",
    "#Unpruned Decision Trees\n",
    "dt_scores = cross_val_score(DecisionTreeClassifier(random_state = rs), X_noise, y, cv=KFold(10))\n",
    "#print(dt_scores)\n",
    "print(f\"Decision Tree Mean test accuracy: {np.mean(dt_scores):.3f}\")\n",
    "means += [dt_scores]\n",
    "\n",
    "std = stdev(dt_scores)\n",
    "sds += [std]\n",
    "print(f\"Decision Tree Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Random Forest\n",
    "rf_scores = cross_val_score(RandomForestClassifier(random_state = rs), X_noise, y, cv=KFold(10))\n",
    "#print(rf_scores)\n",
    "print(f\"Random Forest Mean test accuracy: {np.mean(rf_scores):.3f}\")\n",
    "means += [rf_scores]\n",
    "\n",
    "std = stdev(rf_scores)\n",
    "sds += [std]\n",
    "print(f\"Random Forest Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Decision Stumps\n",
    "dts_scores = cross_val_score(DecisionTreeClassifier(max_depth = 1, random_state = rs), X_noise, y, cv=KFold(10))\n",
    "#print(dt_scores)\n",
    "print(f\"Decision Stump Mean test accuracy: {np.mean(dts_scores):.3f}\")\n",
    "means += [dts_scores]\n",
    "\n",
    "std = stdev(dts_scores)\n",
    "sds += [std]\n",
    "print(f\"Decision Stump Standard Deviation: {std:.3f}\")\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Decision Tree Mean test accuracy: 0.599\n",
      "Pruned Decision Tree Standard Deviation: 0.041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = pd.DataFrame(y)\n",
    "#Pruned Decision Tree\n",
    "kf = KFold(n_splits = 10, shuffle = True)\n",
    "pdt = DecisionTreeClassifier(ccp_alpha = best_ccp_alpha, random_state = rs)\n",
    "\n",
    "pdt_scores = []\n",
    "for i in range(10):\n",
    "    result = next(kf.split(X_noise), None)\n",
    "    X_train = X_noise.iloc[result[0]]\n",
    "    X_test = X_noise.iloc[result[1]]\n",
    "    y_train = y.iloc[result[0]]\n",
    "    y_test = y.iloc[result[1]]\n",
    "    model = pdt.fit(X_train, y_train)\n",
    "    predictions = pdt.predict(X_test)\n",
    "    pdt_scores.append(model.score(X_test, y_test))\n",
    "\n",
    "#print('Scores from each Iteration: ', pdt_scores)\n",
    "print('Pruned Decision Tree Mean test accuracy:', np.mean(pdt_scores))\n",
    "means += [np.mean(pdt_scores)]\n",
    "\n",
    "std = stdev(pdt_scores)\n",
    "sds += [std]\n",
    "print(f\"Pruned Decision Tree Standard Deviation: {std:.3f}\")\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                      meanrank      mean       std  ci_lower  ci_upper  \\\n",
      "Random Forest              1.3  0.703513  0.053407  0.650973  0.756053   \n",
      "Decision Stumps            2.6  0.603753  0.039493  0.564901  0.642605   \n",
      "Pruned Decision Tree       3.0  0.592255  0.035760  0.557076  0.627434   \n",
      "Decision Tree              3.1  0.578865  0.097291  0.483154  0.674577   \n",
      "\n",
      "                     effect_size   magnitude  \n",
      "Random Forest                  0  negligible  \n",
      "Decision Stumps          2.12401       large  \n",
      "Pruned Decision Tree     2.44804       large  \n",
      "Decision Tree             1.5883       large  \n",
      "pvalue=0.006246400552273741\n",
      "cd=1.483221853685529\n",
      "omnibus=friedman\n",
      "posthoc=nemenyi\n",
      "all_normal=True\n",
      "pvals_shapiro=[0.3412385582923889, 0.8706462979316711, 0.5630434155464172, 0.8345291018486023]\n",
      "homoscedastic=False\n",
      "pval_homogeneity=0.00910950246508038\n",
      "homogeneity_test=bartlett\n",
      "alpha=0.05\n",
      "alpha_normality=0.0125\n",
      "num_samples=10\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=cohen_d)\n"
     ]
    }
   ],
   "source": [
    "#Autorank\n",
    "classifiers = ['Decision Tree', 'Random Forest', 'Decision Stumps', 'Pruned Decision Tree']\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for i in range(4):\n",
    "     data[classifiers[i]] = np.random.normal(means[i], sds[i], 10).clip(0, 1)\n",
    "\n",
    "result = autorank(data, alpha=0.05, verbose=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would make sense that the Decision tree would perform better with noisy data given that it would usually overfit on the clean data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Five"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplicative Noise\n",
    "* Repeat step 4 with 20% normal multiplicative noise. Additionally, explain why you think additive and multiplicative noise influence the classifiers differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplicative normal noise\n",
    "noise = np.random.normal(1, 0.2, np.shape(X))\n",
    "X_noise = np.multiply(X, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=1, random_state=8910)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = 8910\n",
    "\n",
    "#Train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_noise, y, test_size=0.3, random_state = rs)\n",
    "\n",
    "#Random Forest\n",
    "rf = RandomForestClassifier(random_state = rs)\n",
    "rf.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "#Unpruned Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state = rs)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#Decision stump\n",
    "dts = DecisionTreeClassifier(max_depth = 1, random_state = rs)\n",
    "dts.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy on train set: 1.00\n",
      "Decision Tree Accuracy on test set: 0.64\n",
      "\n",
      "Random Forest Accuracy on train set: 1.00\n",
      "Random Forest Accuracy on test set: 0.71\n",
      "\n",
      "Decision Stump Accuracy on train set: 0.63\n",
      "Decision Stump Accuracy on test set: 0.59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Scores\n",
    "\n",
    "#Unpruned Decision Tree Score\n",
    "dt_score = dt.score(X_train, y_train)\n",
    "print(f'Decision Tree Accuracy on train set: {dt_score:.2f}')\n",
    "\n",
    "dt_score = dt.score(X_test, y_test)\n",
    "print(f'Decision Tree Accuracy on test set: {dt_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Random Forest Score\n",
    "rf_score = rf.score(X_train, y_train)\n",
    "print(f'Random Forest Accuracy on train set: {rf_score:.2f}')\n",
    "\n",
    "rf_score = rf.score(X_test, y_test)\n",
    "print(f'Random Forest Accuracy on test set: {rf_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Decision Stump Score\n",
    "dts_score = dts.score(X_train, y_train)\n",
    "print(f'Decision Stump Accuracy on train set: {dts_score:.2f}')\n",
    "\n",
    "dts_score = dts.score(X_test, y_test)\n",
    "print(f'Decision Stump Accuracy on test set: {dts_score:.2f}')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Mean test accuracy: 0.633\n",
      "Decision Tree Standard Deviation: 0.053\n",
      "\n",
      "Random Forest Mean test accuracy: 0.691\n",
      "Random Forest Standard Deviation: 0.036\n",
      "\n",
      "Decision Stump Mean test accuracy: 0.596\n",
      "Decision Stump Standard Deviation: 0.035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cross Validation\n",
    "means = []\n",
    "sds = []\n",
    "y = np.ravel(y)\n",
    "\n",
    "#Unpruned Decision Trees\n",
    "dt_scores = cross_val_score(DecisionTreeClassifier(random_state = rs), X_noise, y, cv=KFold(10))\n",
    "#print(dt_scores)\n",
    "print(f\"Decision Tree Mean test accuracy: {np.mean(dt_scores):.3f}\")\n",
    "means += [dt_scores]\n",
    "\n",
    "std = stdev(dt_scores)\n",
    "sds += [std]\n",
    "print(f\"Decision Tree Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Random Forest\n",
    "rf_scores = cross_val_score(RandomForestClassifier(random_state = rs), X_noise, y, cv=KFold(10))\n",
    "#print(rf_scores)\n",
    "print(f\"Random Forest Mean test accuracy: {np.mean(rf_scores):.3f}\")\n",
    "means += [rf_scores]\n",
    "\n",
    "std = stdev(rf_scores)\n",
    "sds += [std]\n",
    "print(f\"Random Forest Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Decision Stumps\n",
    "dts_scores = cross_val_score(DecisionTreeClassifier(max_depth = 1, random_state = rs), X_noise, y, cv=KFold(10))\n",
    "#print(dt_scores)\n",
    "print(f\"Decision Stump Mean test accuracy: {np.mean(dts_scores):.3f}\")\n",
    "means += [dts_scores]\n",
    "\n",
    "std = stdev(dts_scores)\n",
    "sds += [std]\n",
    "print(f\"Decision Stump Standard Deviation: {std:.3f}\")\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Decision Tree Mean test accuracy: 0.617\n",
      "Pruned Decision Tree Standard Deviation: 0.050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = pd.DataFrame(y)\n",
    "#Pruned Decision Tree\n",
    "kf = KFold(n_splits = 10, shuffle = True)\n",
    "pdt = DecisionTreeClassifier(ccp_alpha = best_ccp_alpha, random_state = rs)\n",
    "\n",
    "pdt_scores = []\n",
    "for i in range(10):\n",
    "    result = next(kf.split(X_noise), None)\n",
    "    X_train = X_noise.iloc[result[0]]\n",
    "    X_test = X_noise.iloc[result[1]]\n",
    "    y_train = y.iloc[result[0]]\n",
    "    y_test = y.iloc[result[1]]\n",
    "    model = pdt.fit(X_train, y_train)\n",
    "    predictions = pdt.predict(X_test)\n",
    "    pdt_scores.append(model.score(X_test, y_test))\n",
    "\n",
    "#print('Scores from each Iteration: ', pdt_scores)\n",
    "print('Pruned Decision Tree Mean test accuracy:', np.mean(pdt_scores))\n",
    "means += [np.mean(pdt_scores)]\n",
    "\n",
    "std = stdev(pdt_scores)\n",
    "sds += [std]\n",
    "print(f\"Pruned Decision Tree Standard Deviation: {std:.3f}\")\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janin\\Anaconda3\\lib\\site-packages\\statsmodels\\sandbox\\stats\\multicomp.py:775: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax1.set_yticklabels(np.insert(self.groupsunique.astype(str), 0, ''))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                       meanrank      mean       std  ci_lower  ci_upper  \\\n",
      "Random Forest               1.9  0.674488  0.054919  0.639471  0.709505   \n",
      "Decision Tree               2.6  0.634482  0.069610  0.599465    0.6695   \n",
      "Pruned Decision Trees       2.6  0.605374  0.062716  0.570357  0.640391   \n",
      "Decision Stumps             2.9  0.604249  0.041579  0.569232  0.639266   \n",
      "\n",
      "                      effect_size   magnitude  \n",
      "Random Forest                   0  negligible  \n",
      "Decision Tree            0.638092      medium  \n",
      "Pruned Decision Trees     1.17249       large  \n",
      "Decision Stumps           1.44206       large  \n",
      "pvalue=0.041358914536295366\n",
      "cd=None\n",
      "omnibus=anova\n",
      "posthoc=tukeyhsd\n",
      "all_normal=True\n",
      "pvals_shapiro=[0.5078120231628418, 0.3019663691520691, 0.616109311580658, 0.46593230962753296]\n",
      "homoscedastic=True\n",
      "pval_homogeneity=0.5022449262889062\n",
      "homogeneity_test=bartlett\n",
      "alpha=0.05\n",
      "alpha_normality=0.0125\n",
      "num_samples=10\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=cohen_d)\n"
     ]
    }
   ],
   "source": [
    "#Autorank\n",
    "classifiers = ['Decision Tree', 'Random Forest', 'Decision Stumps', 'Pruned Decision Trees']\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for i in range(4):\n",
    "     data[classifiers[i]] = np.random.normal(means[i], sds[i], 10).clip(0, 1)\n",
    "\n",
    "result = autorank(data, alpha=0.05, verbose=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Six"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Noise\n",
    "* Use 5% class noise (that is, you flip 5% of the labels to the other class) and investigate which classifiers’ performance is affected significantly. Why do you think class noise affects the classifiers differently from feature noise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y = np.ravel(y)\n",
    "flip = np.random.binomial(1, 0.05, y.shape).astype(bool)\n",
    "y_flipped = np.where(flip, 1 - y, y)\n",
    "print(np.c_[y, y_flipped][flip])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=1, random_state=1112)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = 1112\n",
    "\n",
    "#Train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_flipped, test_size=0.3)\n",
    "\n",
    "#Random Forest\n",
    "rf = RandomForestClassifier(random_state = rs)\n",
    "rf.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "#Unpruned Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state = rs)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#Decision stump\n",
    "dts = DecisionTreeClassifier(max_depth = 1, random_state = rs)\n",
    "dts.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy on train set: 1.00\n",
      "Decision Tree Accuracy on test set: 0.52\n",
      "\n",
      "Random Forest Accuracy on train set: 1.00\n",
      "Random Forest Accuracy on test set: 0.54\n",
      "\n",
      "Decision Stump Accuracy on train set: 0.61\n",
      "Decision Stump Accuracy on test set: 0.50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Scores\n",
    "\n",
    "#Unpruned Decision Tree Score\n",
    "dt_score = dt.score(X_train, y_train)\n",
    "print(f'Decision Tree Accuracy on train set: {dt_score:.2f}')\n",
    "\n",
    "dt_score = dt.score(X_val, y_test)\n",
    "print(f'Decision Tree Accuracy on test set: {dt_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Random Forest Score\n",
    "rf_score = rf.score(X_train, y_train)\n",
    "print(f'Random Forest Accuracy on train set: {rf_score:.2f}')\n",
    "\n",
    "rf_score = rf.score(X_val, y_test)\n",
    "print(f'Random Forest Accuracy on test set: {rf_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Decision Stump Score\n",
    "dts_score = dts.score(X_train, y_train)\n",
    "print(f'Decision Stump Accuracy on train set: {dts_score:.2f}')\n",
    "\n",
    "dts_score = dts.score(X_val, y_test)\n",
    "print(f'Decision Stump Accuracy on test set: {dts_score:.2f}')\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Mean test accuracy: 0.580\n",
      "Decision Tree Standard Deviation: 0.045\n",
      "\n",
      "Random Forest Mean test accuracy: 0.661\n",
      "Random Forest Standard Deviation: 0.040\n",
      "\n",
      "Decision Stump Mean test accuracy: 0.579\n",
      "Decision Stump Standard Deviation: 0.027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cross Validation\n",
    "means = []\n",
    "sds = []\n",
    "\n",
    "#Unpruned Decision Trees\n",
    "dt_scores = cross_val_score(DecisionTreeClassifier(random_state = rs), X, y_flipped, cv=KFold(10))\n",
    "#print(dt_scores)\n",
    "print(f\"Decision Tree Mean test accuracy: {np.mean(dt_scores):.3f}\")\n",
    "means += [dt_scores]\n",
    "\n",
    "std = stdev(dt_scores)\n",
    "sds += [std]\n",
    "print(f\"Decision Tree Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Random Forest\n",
    "rf_scores = cross_val_score(RandomForestClassifier(random_state = rs), X, y_flipped, cv=KFold(10))\n",
    "#print(rf_scores)\n",
    "print(f\"Random Forest Mean test accuracy: {np.mean(rf_scores):.3f}\")\n",
    "means += [rf_scores]\n",
    "\n",
    "std = stdev(rf_scores)\n",
    "sds += [std]\n",
    "print(f\"Random Forest Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Decision Stumps\n",
    "dts_scores = cross_val_score(DecisionTreeClassifier(max_depth = 1, random_state = rs), X, y_flipped, cv=KFold(10))\n",
    "#print(dt_scores)\n",
    "print(f\"Decision Stump Mean test accuracy: {np.mean(dts_scores):.3f}\")\n",
    "means += [dts_scores]\n",
    "\n",
    "std = stdev(dts_scores)\n",
    "sds += [std]\n",
    "print(f\"Decision Stump Standard Deviation: {std:.3f}\")\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Decision Tree Mean test accuracy: 0.594\n",
      "Pruned Decision Tree Standard Deviation: 0.034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_flipped = pd.DataFrame(y_flipped)\n",
    "#Pruned Decision Tree\n",
    "kf = KFold(n_splits = 10, shuffle = True)\n",
    "pdt = DecisionTreeClassifier(ccp_alpha = best_ccp_alpha, random_state = rs)\n",
    "\n",
    "pdt_scores = []\n",
    "for i in range(10):\n",
    "    result = next(kf.split(X_noise), None)\n",
    "    X_train = X.iloc[result[0]]\n",
    "    X_test = X.iloc[result[1]]\n",
    "    y_train = y_flipped.iloc[result[0]]\n",
    "    y_test = y_flipped.iloc[result[1]]\n",
    "    model = pdt.fit(X_train, y_train)\n",
    "    predictions = pdt.predict(X_test)\n",
    "    pdt_scores.append(model.score(X_test, y_test))\n",
    "\n",
    "#print('Scores from each Iteration: ', pdt_scores)\n",
    "print('Pruned Decision Tree Mean test accuracy:', np.mean(pdt_scores))\n",
    "means += [np.mean(pdt_scores)]\n",
    "\n",
    "std = stdev(pdt_scores)\n",
    "sds += [std]\n",
    "print(f\"Pruned Decision Tree Standard Deviation: {std:.3f}\")\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janin\\Anaconda3\\lib\\site-packages\\statsmodels\\sandbox\\stats\\multicomp.py:775: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax1.set_yticklabels(np.insert(self.groupsunique.astype(str), 0, ''))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                       meanrank      mean       std  ci_lower  ci_upper  \\\n",
      "Random Forest               1.5  0.645182  0.076839  0.609736  0.680627   \n",
      "Pruned Decision Trees       2.7  0.589153  0.030703  0.553707  0.624598   \n",
      "Decision Tree               2.9  0.591672  0.068819  0.556226  0.627117   \n",
      "Decision Stumps             2.9  0.580831  0.047684  0.545386  0.616277   \n",
      "\n",
      "                      effect_size   magnitude  \n",
      "Random Forest                   0  negligible  \n",
      "Pruned Decision Trees    0.957601       large  \n",
      "Decision Tree            0.733623      medium  \n",
      "Decision Stumps           1.00634       large  \n",
      "pvalue=0.04444426074756692\n",
      "cd=None\n",
      "omnibus=anova\n",
      "posthoc=tukeyhsd\n",
      "all_normal=True\n",
      "pvals_shapiro=[0.5800461769104004, 0.14320023357868195, 0.31138208508491516, 0.585840106010437]\n",
      "homoscedastic=True\n",
      "pval_homogeneity=0.056523377705134714\n",
      "homogeneity_test=bartlett\n",
      "alpha=0.05\n",
      "alpha_normality=0.0125\n",
      "num_samples=10\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=cohen_d)\n"
     ]
    }
   ],
   "source": [
    "#Autorank\n",
    "classifiers = ['Decision Tree', 'Random Forest', 'Decision Stumps', 'Pruned Decision Trees']\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for i in range(4):\n",
    "     data[classifiers[i]] = np.random.normal(means[i], sds[i], 10).clip(0, 1)\n",
    "\n",
    "result = autorank(data, alpha=0.05, verbose=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Seven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you split the data into a training and test set first, and only afterwards add 20% normal multiplicative noise to the training set, how differently does your algorithm behave? Try this again and add noise only to the test set. Are the results different? Discuss how each of these approaches affect your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=1, random_state=121314)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = 121314\n",
    "\n",
    "#Train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "#Noise\n",
    "noise = np.random.normal(1, 0.2, np.shape(X_test))\n",
    "X_test = np.multiply(X_test, noise)\n",
    "\n",
    "#Random Forest\n",
    "rf = RandomForestClassifier(random_state = rs)\n",
    "rf.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "#Unpruned Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state = rs)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#Decision stump\n",
    "dts = DecisionTreeClassifier(max_depth = 1, random_state = rs)\n",
    "dts.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy on train set: 1.00\n",
      "Decision Tree Accuracy on test set: 0.61\n",
      "\n",
      "Random Forest Accuracy on train set: 1.00\n",
      "Random Forest Accuracy on test set: 0.73\n",
      "\n",
      "Decision Stump Accuracy on train set: 0.63\n",
      "Decision Stump Accuracy on test set: 0.57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Scores\n",
    "\n",
    "#Unpruned Decision Tree Score\n",
    "dt_score = dt.score(X_train, y_train)\n",
    "print(f'Decision Tree Accuracy on train set: {dt_score:.2f}')\n",
    "\n",
    "dt_score = dt.score(X_test, y_test)\n",
    "print(f'Decision Tree Accuracy on test set: {dt_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Random Forest Score\n",
    "rf_score = rf.score(X_train, y_train)\n",
    "print(f'Random Forest Accuracy on train set: {rf_score:.2f}')\n",
    "\n",
    "rf_score = rf.score(X_test, y_test)\n",
    "print(f'Random Forest Accuracy on test set: {rf_score:.2f}')\n",
    "print('')\n",
    "\n",
    "#Decision Stump Score\n",
    "dts_score = dts.score(X_train, y_train)\n",
    "print(f'Decision Stump Accuracy on train set: {dts_score:.2f}')\n",
    "\n",
    "dts_score = dts.score(X_test, y_test)\n",
    "print(f'Decision Stump Accuracy on test set: {dts_score:.2f}')\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Mean test accuracy: 0.6589999999999999\n",
      "Decision Tree Standard Deviation: 0.034\n",
      "\n",
      "Random Forest Mean test accuracy: 0.724\n",
      "Random Forest Standard Deviation: 0.044\n",
      "\n",
      "Decision Tree Mean test accuracy: 0.615\n",
      "Decision Tree Standard Deviation: 0.049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "means = []\n",
    "sds = []\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)\n",
    "\n",
    "#Decision Tree\n",
    "kf = KFold(n_splits = 10, shuffle = True)\n",
    "dt = (DecisionTreeClassifier(random_state = rs))\n",
    "\n",
    "dt_scores = []\n",
    "for i in range(10):\n",
    "    result = next(kf.split(X), None)\n",
    "    X_train = X.iloc[result[0]]\n",
    "    X_test = X.iloc[result[1]]\n",
    "    noise = np.random.normal(1, 0.2, np.shape(X_test))\n",
    "    X_test = np.multiply(X_test, noise)\n",
    "    \n",
    "    y_train = y.iloc[result[0]]\n",
    "    y_test = y.iloc[result[1]]\n",
    "    model = dt.fit(X_train, y_train)\n",
    "    predictions = dt.predict(X_test)\n",
    "    dt_scores.append(model.score(X_test, y_test))\n",
    "\n",
    "#print('Scores from each Iteration: ', pdt_scores)\n",
    "print('Decision Tree Mean test accuracy:', np.mean(dt_scores))\n",
    "means += [np.mean(dt_scores)]\n",
    "\n",
    "std = stdev(dt_scores)\n",
    "sds += [std]\n",
    "print(f\"Decision Tree Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#y = np.ravel(y)\n",
    "#Random Forest\n",
    "rf = (RandomForestClassifier(random_state = rs))\n",
    "\n",
    "rf_scores = []\n",
    "for i in range(10):\n",
    "    result = next(kf.split(X), None)\n",
    "    X_train = X.iloc[result[0]]\n",
    "    X_test = X.iloc[result[1]]\n",
    "    noise = np.random.normal(1, 0.2, np.shape(X_test))\n",
    "    X_test = np.multiply(X_test, noise)\n",
    "    \n",
    "    y_train = y.iloc[result[0]]\n",
    "    y_train = np.ravel(y_train)\n",
    "    y_test = y.iloc[result[1]]\n",
    "    model = rf.fit(X_train, y_train)\n",
    "    predictions = rf.predict(X_test)\n",
    "    rf_scores.append(model.score(X_test, y_test))\n",
    "\n",
    "#print('Scores from each Iteration: ', pdt_scores)\n",
    "print('Random Forest Mean test accuracy:', np.mean(rf_scores))\n",
    "means += [np.mean(rf_scores)]\n",
    "\n",
    "std = stdev(rf_scores)\n",
    "sds += [std]\n",
    "print(f\"Random Forest Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n",
    "#Decision Stump\n",
    "ds = (DecisionTreeClassifier(max_depth = 1, random_state = rs))\n",
    "\n",
    "ds_scores = []\n",
    "for i in range(10):\n",
    "    result = next(kf.split(X), None)\n",
    "    X_train = X.iloc[result[0]]\n",
    "    X_test = X.iloc[result[1]]\n",
    "    noise = np.random.normal(1, 0.2, np.shape(X_test))\n",
    "    X_test = np.multiply(X_test, noise)\n",
    "    \n",
    "    y_train = y.iloc[result[0]]\n",
    "    y_test = y.iloc[result[1]]\n",
    "    y_train = np.ravel(y_train)\n",
    "    model = ds.fit(X_train, y_train)\n",
    "    predictions = ds.predict(X_test)\n",
    "    ds_scores.append(model.score(X_test, y_test))\n",
    "\n",
    "#print('Scores from each Iteration: ', pdt_scores)\n",
    "print('Decision Tree Mean test accuracy:', np.mean(ds_scores))\n",
    "means += [np.mean(ds_scores)]\n",
    "\n",
    "std = stdev(ds_scores)\n",
    "sds += [std]\n",
    "print(f\"Decision Tree Standard Deviation: {std:.3f}\")\n",
    "print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Decision Tree Mean test accuracy: 0.619\n",
      "Pruned Decision Tree Standard Deviation: 0.055\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Pruned Decision Tree\n",
    "pdt = DecisionTreeClassifier(ccp_alpha = best_ccp_alpha, random_state = rs)\n",
    "\n",
    "pdt_scores = []\n",
    "for i in range(10):\n",
    "    result = next(kf.split(X_noise), None)\n",
    "    X_train = X.iloc[result[0]]\n",
    "    X_test = X.iloc[result[1]]\n",
    "    noise = np.random.normal(1, 0.2, np.shape(X_test))\n",
    "    X_test = np.multiply(X_test, noise)\n",
    "    \n",
    "    \n",
    "    y_train = y.iloc[result[0]]\n",
    "    y_test = y.iloc[result[1]]\n",
    "    model = pdt.fit(X_train, y_train)\n",
    "    predictions = pdt.predict(X_test)\n",
    "    pdt_scores.append(model.score(X_test, y_test))\n",
    "\n",
    "#print('Scores from each Iteration: ', pdt_scores)\n",
    "print('Pruned Decision Tree Mean test accuracy:', np.mean(pdt_scores))\n",
    "means += [np.mean(pdt_scores)]\n",
    "\n",
    "std = stdev(pdt_scores)\n",
    "sds += [std]\n",
    "print(f\"Pruned Decision Tree Standard Deviation: {std:.3f}\")\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "                       meanrank      mean       std  ci_lower  ci_upper  \\\n",
      "Random Forest               1.2  0.730022  0.037257  0.703174  0.756869   \n",
      "Decision Tree               2.5  0.658249  0.023950  0.631402  0.685097   \n",
      "Decision Stumps             3.0  0.615517  0.048870  0.588669  0.642364   \n",
      "Pruned Decision Trees       3.3  0.612904  0.059996  0.586056  0.639751   \n",
      "\n",
      "                      effect_size   magnitude  \n",
      "Random Forest                   0  negligible  \n",
      "Decision Tree              2.2917       large  \n",
      "Decision Stumps           2.63514       large  \n",
      "Pruned Decision Trees     2.34527       large  \n",
      "pvalue=8.299012785904896e-06\n",
      "cd=None\n",
      "omnibus=anova\n",
      "posthoc=tukeyhsd\n",
      "all_normal=True\n",
      "pvals_shapiro=[0.3737439513206482, 0.1279202401638031, 0.6751849055290222, 0.9680892825126648]\n",
      "homoscedastic=True\n",
      "pval_homogeneity=0.06921407977297021\n",
      "homogeneity_test=bartlett\n",
      "alpha=0.05\n",
      "alpha_normality=0.0125\n",
      "num_samples=10\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=cohen_d)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janin\\Anaconda3\\lib\\site-packages\\statsmodels\\sandbox\\stats\\multicomp.py:775: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax1.set_yticklabels(np.insert(self.groupsunique.astype(str), 0, ''))\n"
     ]
    }
   ],
   "source": [
    "#Autorank\n",
    "classifiers = ['Decision Tree', 'Random Forest', 'Decision Stumps', 'Pruned Decision Trees']\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for i in range(4):\n",
    "     data[classifiers[i]] = np.random.normal(means[i], sds[i], 10).clip(0, 1)\n",
    "\n",
    "result = autorank(data, alpha=0.05, verbose=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
